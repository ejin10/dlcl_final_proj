{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "19d14290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "from datasets import Dataset, load_from_disk, concatenate_datasets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizerFast, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49f71838",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 500\n",
    "load_top_n = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9c56835",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_finetuned_bert = '/home/ubuntu/partisan_bias_detection/best_models/final_models/final_bert'\n",
    "path_to_pretrained_bert = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2fa7123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/datasets/dataset_dict.py:1241: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "CWD = os.getcwd()\n",
    "DATASET_DIR = os.path.join(CWD, 'data')\n",
    "\n",
    "dataset = load_from_disk(DATASET_DIR)\n",
    "dataset = dataset.rename_column('label', 'labels')\n",
    "\n",
    "# ds = concatenate_datasets([dataset['train'], dataset['val'], dataset['test']]).shuffle(seed=100).select(range(500000))\n",
    "ds = dataset['train']\n",
    "ds = ds.remove_columns('input_ids')\n",
    "ds = ds.to_pandas()\n",
    "\n",
    "left = Dataset.from_pandas(ds.loc[ds['labels'] == 4])\n",
    "leftcenter = Dataset.from_pandas(ds.loc[ds['labels'] == 3])\n",
    "neutral = Dataset.from_pandas(ds.loc[ds['labels'] == 2])\n",
    "rightcenter = Dataset.from_pandas(ds.loc[ds['labels'] == 1])\n",
    "right = Dataset.from_pandas(ds.loc[ds['labels'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dff92dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 500\n",
      "left: ['bush', 'climate', 'israeli', 'romney', 'movement', 'gop', 'let', 'actually', 'afghanistan', 'course', 'weapons', 'politics', 'thing', 'labor', 'ever', 'class', 'nothing', 'book', 'mccain', 'middle', 'question', 'conservative', 'palestinian', 'civil', 'corporate', 'intelligence', 'army', 'abortion', 'speech', 'peace', 'debate', 'action', 'iraqi', 'candidate', 'democracy', 'kind', 'watch', 'lt', 'legal', 'clear', 'sanders', 'private', 'almost', 'isn', 'given', 'george', 'gt', 'troops', 'having', 'seems', 'hard', 'done', 'rather', 'via', 'mother', 'whose', 'paul', 'reform', 'means', 'instead', 'face', 'stop', 'evidence', 'talk', 'himself', 'bad', 'words', 'immigration']\n",
      "\n",
      "left_center: ['baptist', 'globalpost', 'god', 'lgbt', 'chicago', 'religious', 'teachers', 'study', 'pastor', 'churches', 'virginia', 'india', 'brown', 'southern', 'ministry', 'christian', 'love', 'music', 'known', 'chinese', 'parents', 'ukraine', 'convention', 'faith', 'research', 'bbc', 'baptists', 'student', 'mission', 'syrian', 'jesus', 'nbc', 'person', 'council', 'africa', 'name', 'families', 'together', 'recently', 'building', 'african', 'able', 'photo', 'comes']\n",
      "\n",
      "neutral: ['ap', 'albuquerque', 'numbers', 'winning', 'twenty', 'santa', 'evening', 'fe', 'seven', 'eight', 'nine', 'lottery', '___', 'pick', 'jackpot', 'estimated', 'thirty', 'drawing', 'attorney', 'journal', 'rio', 'unm', 'officer', 'shot', 'judge', 'coach', 'shooting', 'car', 'half', 'release', 'charges', 'martinez', 'drawn', 'arrested', 'morning', 'hospital', 'medical', 'colorado', 'los', 'sheriff', 'vehicle', '40', 'park', 'rancho', '19', 'arizona', '21', 'ball', '24', 'lucky', 'st', '22', 'died', 'road', 'conference', 'scored', 'prison', 'charged', '23', 'governor', 'sum', 'along', 'abuse', 'lobos', 'spokesman']\n",
      "\n",
      "right_center: ['fusion', 'possible', 'website', 'within', 'result', 'loss', 'involved', 'damage', 'film', 'sell', 'contained', 'anyone', 'risks', 'fully', 'regarding', 'accept', 'please', 'informed', 'forms', 'signals', 'charts', 'liability', 'reliance', 'quotes', 'riskiest', 'upi', 'series', 'published', 'hurricane', 'star', 'european', 'apos', 'nevada', 'yards', '2018', 'eu', 'korean', 'performance', 'central', 'missile', 'aug', 'nfl', 'august', 'shows', 'nov', 'vegas', 'storm', 'return', 'saudi']\n",
      "\n",
      "right: ['opens', 'window', 'shares', 'fool', 'below', 'continue', 'investors', 'reading', 'motley', 'revenue', 'apple', 'credit', 'cents', 'income', 'nyse', 'average', 'facebook', '500', 'nasdaq', 'index', 'debt', 'profit', 'rates', 'source', 'dow', 'interest', 'results', 'fed', 'term', 'marketwatch', 'amazon', 'dividend', 'position', 'david', 'ceo', 'customers', 'management', 'net', 'click', 'jones', 'technology', 'retirement', 'capital', 'strong', 'consumer', 'cost', 'businesses', 'tom', 'disclosure', 'makes', 'google', 'demand', 'employees', 'content', 'products', 'rise', 'line', 'advisor', 'biggest', 'value', 'fund', 'image', 'card', 'corp', 'period', 'store', 'compared', 'coming', '100', 'largest', 'total', 'copyright', 'reporting', 'june', 'gains', 'range', 'futures', 'cut', 'mentioned', 'short', 'gold', 'looking', 'expectations', 'banks', 'crude']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if load_top_n:\n",
    "    with open(f'top_{top_n}_words.pkl', 'rb') as f:\n",
    "        top_words = pkl.load(f)\n",
    "else:\n",
    "    vec = TfidfVectorizer()\n",
    "\n",
    "    print('fitting left')\n",
    "    left_scores = vec.fit_transform(left['text'])\n",
    "    top_left = sorted(list(zip(vec.get_feature_names(), left_scores.sum(0).getA1())), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_left = [word for word, score in top_left]\n",
    "\n",
    "    print('fitting leftcenter')\n",
    "    leftcenter_scores = vec.fit_transform(leftcenter['text'])\n",
    "    top_leftcenter = sorted(list(zip(vec.get_feature_names(), leftcenter_scores.sum(0).getA1())), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_leftcenter = [word for word, score in top_leftcenter]\n",
    "\n",
    "    print('fitting neutral')\n",
    "    neutral_scores = vec.fit_transform(neutral['text'])\n",
    "    top_neutral = sorted(list(zip(vec.get_feature_names(), neutral_scores.sum(0).getA1())), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_neutral = [word for word, score in top_neutral]\n",
    "\n",
    "    print('rightcenter neutral')\n",
    "    rightcenter_scores = vec.fit_transform(rightcenter['text'])\n",
    "    top_rightcenter = sorted(list(zip(vec.get_feature_names(), rightcenter_scores.sum(0).getA1())), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_rightcenter = [word for word, score in top_rightcenter]\n",
    "\n",
    "    print('fitting right')\n",
    "    right_scores = vec.fit_transform(right['text'])\n",
    "    top_right = sorted(list(zip(vec.get_feature_names(), right_scores.sum(0).getA1())), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_right = [word for word, score in top_right]\n",
    "    \n",
    "    common = list(\n",
    "            set(top_left).intersection(set(top_leftcenter), set(top_neutral), set(top_rightcenter), set(top_right)).union(\n",
    "                set(top_left).intersection(set(top_leftcenter)).union(\n",
    "                set(top_left).intersection(set(top_neutral)).union(\n",
    "                set(top_left).intersection(set(top_rightcenter)).union(\n",
    "                set(top_left).intersection(set(top_right)).union(\n",
    "                    set(top_leftcenter).intersection(set(top_neutral)).union(\n",
    "                    set(top_leftcenter).intersection(set(top_rightcenter)).union(\n",
    "                    set(top_leftcenter).intersection(set(top_right)).union(\n",
    "                        set(top_neutral).intersection(set(top_rightcenter)).union(\n",
    "                        set(top_neutral).intersection(set(top_right)).union(\n",
    "                            set(top_rightcenter).intersection(set(top_right))\n",
    "                        )\n",
    "                        )\n",
    "                    )\n",
    "                    )\n",
    "                    )\n",
    "                )\n",
    "                )\n",
    "                )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # print(f'common: \\n', common)\n",
    "\n",
    "    for word in common:\n",
    "        if word in top_left:\n",
    "            top_left.remove(word)\n",
    "        if word in top_leftcenter:\n",
    "            top_leftcenter.remove(word)\n",
    "        if word in top_neutral:\n",
    "            top_neutral.remove(word)\n",
    "        if word in top_rightcenter:\n",
    "            top_rightcenter.remove(word)\n",
    "        if word in top_right:\n",
    "            top_right.remove(word)\n",
    "\n",
    "    print(f'left top {top_n}: \\n', top_left)\n",
    "    print(f'left-center top {top_n}: \\n', top_leftcenter)\n",
    "    print(f'neutral top {top_n}: \\n', top_neutral)\n",
    "    print(f'right-center top {top_n}: \\n', top_rightcenter)\n",
    "    print(f'right top {top_n}: \\n', top_right)\n",
    "\n",
    "    top_words = {'left': top_left, \n",
    "                 'left_center': top_leftcenter, \n",
    "                 'neutral': top_neutral, \n",
    "                 'right_center': top_rightcenter, \n",
    "                 'right': top_right}\n",
    "\n",
    "    with open(f'top_{top_n}_words.pkl', 'wb') as f:\n",
    "        pkl.dump(top_words, f)\n",
    "\n",
    "print(f'top {top_n}')\n",
    "for bias, words in top_words.items():\n",
    "    print(f'{bias}: {words}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59917034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeds(model, all_words, specific_list, pca_dim=50):\n",
    "    tok = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "    embeds = []\n",
    "    \n",
    "    biases = list(all_words.keys())\n",
    "    is_specific = []\n",
    "    for bias in biases:\n",
    "        words = all_words[bias]\n",
    "        is_specific += [f'{bias}_specific' if word in specific_list else bias for word in words]\n",
    "        model = model.to('cuda:0')\n",
    "        input_ = torch.tensor(tok.convert_tokens_to_ids(words), dtype=torch.int).unsqueeze(0).to('cuda:0')\n",
    "        embeds.extend(torch.nn.functional.pad(model(input_).last_hidden_state.squeeze(), pad=(0, 2)).tolist())\n",
    "    \n",
    "    if pca_dim:\n",
    "        pca = PCA(n_components = pca_dim, random_state = 100)\n",
    "        pca_embeds = pca.fit_transform(embeds)\n",
    "        pca_embeds = pca_embeds.tolist()\n",
    "        final_embeds = deepcopy(pca_embeds)\n",
    "    else:\n",
    "        final_embeds = embeds\n",
    "        \n",
    "    class2bias = {i: biases[i] for i in range(len(biases))}\n",
    "    \n",
    "    lengths = [len(all_words[bias]) for bias in biases]\n",
    "    lengths.insert(0, 0) \n",
    "    lengths = np.cumsum(lengths)\n",
    "    top = [all_words[bias] for bias in biases]\n",
    "\n",
    "    j = 0\n",
    "\n",
    "    for i in range(len(final_embeds)):\n",
    "        if i == lengths[j + 1]:\n",
    "            j += 1\n",
    "        final_embeds[i].append(class2bias[j])\n",
    "        final_embeds[i].append(top[j][i - lengths[j]])\n",
    "        final_embeds[i].append(is_specific[i])\n",
    "    \n",
    "    return final_embeds\n",
    "    \n",
    "    \n",
    "def visualize_embeds(final_embeds, specific=[], notes=''):\n",
    "    columns = [f'D{i}' for i in range(len(list(final_embeds.values())[0][0]) - 3)]\n",
    "    columns.append('bias')\n",
    "    columns.append('word')\n",
    "    columns.append('specific')\n",
    "    \n",
    "    wandb.init(project=f'top_embeds_{\"_\".join(list(final_embeds.keys()))}', notes=notes)\n",
    "    for model_type, embeds in final_embeds.items():\n",
    "        final_embeds[model_type] = wandb.Table(\n",
    "            columns = columns,\n",
    "            data = embeds\n",
    "        )\n",
    "        \n",
    "    wandb.log(final_embeds)\n",
    "    wandb.finish()    \n",
    "    \n",
    "    \n",
    "def get_embeds_and_visualize(words, specific=[], pretrained=None, finetuned=None, hat=None, pca_dim=50, notes=''):\n",
    "    final_embeds = {}\n",
    "    if pretrained:\n",
    "        final_embeds['pretrained'] = get_embeds(pretrained, words, specific, pca_dim)\n",
    "\n",
    "    if finetuned: \n",
    "        final_embeds['finetuned'] = get_embeds(finetuned, words, specific, pca_dim)\n",
    "        \n",
    "    if hat: \n",
    "        final_embeds['hat'] = get_embeds(hat, words, specific, pca_dim)\n",
    "        \n",
    "    visualize_embeds(final_embeds, notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "200ce7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/ubuntu/partisan_bias_detection/best_models/final_models/final_bert were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Some weights of HATForSequenceClassification were not initialized from the model checkpoint at /home/ubuntu/partisan_bias_detection/best_models/train_full_pipeline/final_full_pipeline and are newly initialized because the shapes did not match:\n",
      "- hi_transformer.embeddings.position_ids: found shape torch.Size([1, 512]) in the checkpoint and torch.Size([1, 4096]) in the model instantiated\n",
      "- hi_transformer.embeddings.word_embeddings.weight: found shape torch.Size([30522, 768]) in the checkpoint and torch.Size([50265, 768]) in the model instantiated\n",
      "- hi_transformer.embeddings.position_embeddings.weight: found shape torch.Size([512, 768]) in the checkpoint and torch.Size([130, 768]) in the model instantiated\n",
      "- hi_transformer.embeddings.token_type_embeddings.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = BertModel.from_pretrained(path_to_pretrained_bert)\n",
    "finetuned_model = BertModel.from_pretrained(path_to_finetuned_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fc64a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common left vs. right words\n",
    "specific_words = {\n",
    "    'left': ['rally', 'demonstrations', 'demonstrators', 'protesters', 'termination', 'asylum', 'undocumented'], \n",
    "    'right': ['riots', 'anarchists', 'mob', 'rioters', 'infanticide', 'illegal', 'aliens']\n",
    "}\n",
    "\n",
    "new_words = {}\n",
    "specific_list = []\n",
    "for bias, words in top_words.items():\n",
    "    new_words[bias] = top_words[bias] + specific_words.get(bias, [])\n",
    "    specific_list += specific_words.get(bias, [])\n",
    "\n",
    "get_embeds_and_visualize(new_words, specific_list, pretrained_model, finetuned_model, pca_dim=None, notes='plotting specific words')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
